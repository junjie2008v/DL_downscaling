{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN_initial_attempt\n",
    "\n",
    "Initial experiments of GAN training\n",
    "\n",
    "* how training loops are organized\n",
    "* how to \"level-up\" the descriminator (given that the generator has been fine-tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general tools\n",
    "import sys\n",
    "from glob import glob\n",
    "\n",
    "# data tools\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "\n",
    "# deep learning tools\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# graph tools\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# custom tools\n",
    "sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/utils/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/DL_downscaling/utils/')\n",
    "sys.path.insert(0, '/glade/u/home/ksha/WORKSPACE/DL_downscaling/')\n",
    "from namelist import *\n",
    "import data_utils as du\n",
    "import model_utils as mu\n",
    "import train_utils as tu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'model_utils' from '/glade/u/home/ksha/WORKSPACE/DL_downscaling/utils/model_utils.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(mu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generator**, initialized with pre-trained UNET weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import model: UNET3_TMAX_jja_tune\n",
      "Compiling G\n"
     ]
    }
   ],
   "source": [
    "N = [56, 112, 224, 448] # number of channels per downsampling level\n",
    "l = [1e-4, 2e-4] # G lr; D lr\n",
    "\n",
    "sea = 'jja' # testing in the JJA season\n",
    "N_input = 3 # LR T2, HR elev, LR elev\n",
    "VAR = 'TMAX'\n",
    "\n",
    "\n",
    "model_name = 'UNET{}_{}_{}_tune'.format(N_input, VAR, sea)\n",
    "model_path = temp_dir+model_name+'.hdf'\n",
    "print('Import model: {}'.format(model_name))\n",
    "backbone = keras.models.load_model(model_path)\n",
    "W = backbone.get_weights()\n",
    "\n",
    "# generator\n",
    "G = mu.UNET(N, (None, None, N_input))\n",
    "# optimizer\n",
    "opt_G = keras.optimizers.Adam(lr=l[0])\n",
    "\n",
    "print('Compiling G')\n",
    "G.compile(loss=keras.losses.mean_absolute_error, optimizer=opt_G)\n",
    "G.set_weights(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VGG-like **Descriminator**\n",
    "\n",
    "{G_output, HR_elev, LR_elev} --> {conv56, conv56, maxpool} --> {conv112, conv112, maxpool} --> {conv224, conv224, maxpool} --> {global_pool, sigmoid}\n",
    "\n",
    "* 1/4 size of the G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling D\n"
     ]
    }
   ],
   "source": [
    "# !! test and move to utils\n",
    "def vgg_descriminator(N, input_size):\n",
    "    IN = keras.layers.Input(input_size)\n",
    "\n",
    "    X = mu.CONV_stack(IN, N[0], kernel_size=3, stack_num=2)\n",
    "    X = keras.layers.MaxPooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    X = mu.CONV_stack(X, N[1], kernel_size=3, stack_num=2)\n",
    "    X = keras.layers.MaxPooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    X = mu.CONV_stack(X, N[2], kernel_size=3, stack_num=2)\n",
    "    X = keras.layers.MaxPooling2D(pool_size=(2, 2))(X)\n",
    "\n",
    "    X = keras.layers.GlobalMaxPool2D()(X)\n",
    "    FLAG = keras.layers.Dense(2, activation=keras.activations.sigmoid)(X) #, \n",
    "\n",
    "    return keras.models.Model(inputs=[IN], outputs=[FLAG])\n",
    "\n",
    "    \n",
    "input_size = (None, None, N_input+1)\n",
    "D = vgg_descriminator(N, input_size)\n",
    "\n",
    "opt_D = keras.optimizers.Adam(lr=l[1])\n",
    "print('Compiling D')\n",
    "D.compile(loss=keras.losses.categorical_crossentropy, optimizer=opt_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling GAN\n"
     ]
    }
   ],
   "source": [
    "GAN_IN = keras.layers.Input((None, None, N_input))\n",
    "G_OUT = G(GAN_IN)\n",
    "D_IN = keras.layers.Concatenate()([G_OUT, GAN_IN])\n",
    "D_OUT = D(D_IN)\n",
    "GAN = keras.models.Model(GAN_IN, [G_OUT, D_OUT])\n",
    "\n",
    "print('Compiling GAN')\n",
    "# content_loss + 1e-3 * adversarial_loss\n",
    "GAN.compile(loss=[keras.losses.mean_absolute_error, keras.losses.categorical_crossentropy], \n",
    "            loss_weights=[1.0, 1e-3],\n",
    "            optimizer=opt_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = BATCH_dir\n",
    "trainfiles = glob(file_path+'{}_BATCH_*_TORI*_{}*.npy'.format(VAR, sea)) # e.g., TMAX_BATCH_128_VORIAUG_mam30.npy\n",
    "validfiles = glob(file_path+'{}_BATCH_*_VORI*_{}*.npy'.format(VAR, sea))\n",
    "# shuffle filenames\n",
    "shuffle(trainfiles)\n",
    "shuffle(validfiles)\n",
    "\n",
    "L_train = len(trainfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch = np.load(trainfiles[0], allow_pickle=True)[()]\n",
    "X = temp_batch['batch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3607242122565246"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(X[0, ..., 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_flag = [False, True, False, False, True, True] # LR T2, HR elev, LR elev\n",
    "output_flag = [True, False, False, False, False, False] # HR T2\n",
    "inout_flag = [True, True, False, False, True, True]\n",
    "labels = ['batch', 'batch'] # input and output labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0\n",
      "\t0 step loss = 0.8007091283798218\n",
      "\t100 step loss = 0.6926652789115906\n",
      "\t200 step loss = 0.17872953414916992\n",
      "\t300 step loss = 0.06848559528589249\n",
      "\t400 step loss = 0.05440543219447136\n",
      "\t500 step loss = 0.0018735595513135195\n",
      "\t600 step loss = 0.015836108475923538\n",
      "\t700 step loss = 0.008387360721826553\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e13396ce9a16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0md_shuffle_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle_ind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0md_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_in\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_shuffle_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_shuffle_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#         # G training / transferring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    971\u001b[0m       outputs = training_v2_utils.train_on_batch(\n\u001b[1;32m    972\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 973\u001b[0;31m           class_weight=class_weight, reset_metrics=reset_metrics)\n\u001b[0m\u001b[1;32m    974\u001b[0m       outputs = (outputs['total_loss'] + outputs['output_losses'] +\n\u001b[1;32m    975\u001b[0m                  outputs['metrics'])\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       output_loss_metrics=model._output_loss_metrics)\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    309\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m           \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m           output_loss_metrics=output_loss_metrics))\n\u001b[0m\u001b[1;32m    312\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    250\u001b[0m               \u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_loss_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m               \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m               training=training))\n\u001b[0m\u001b[1;32m    253\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         raise ValueError('The model cannot be run '\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_eager.py\u001b[0m in \u001b[0;36m_model_loss\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m   \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    706\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    707\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    890\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvirtual_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36m_get_training_value\u001b[0;34m(self, training)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trainable_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/glade/work/ksha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mlogical_and\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   5858\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   5859\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_thread_local_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5860\u001b[0;31m         \"LogicalAnd\", name, _ctx._post_execution_callbacks, x, y)\n\u001b[0m\u001b[1;32m   5861\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5862\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_path = temp_dir+'GAN_D_{}_{}.hdf'.format(VAR, sea)\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 200\n",
    "\n",
    "y_bad = np.zeros(batch_size)\n",
    "y_good = np.ones(batch_size)\n",
    "dummy_good = keras.utils.to_categorical(y_good)\n",
    "dummy_mix = keras.utils.to_categorical(np.concatenate((y_bad, y_good), axis=0))\n",
    "\n",
    "input_flag = [False, True, False, False, True, True] # LR T2, HR elev, LR elev\n",
    "output_flag = [True, False, False, False, False, False] # HR T2\n",
    "inout_flag = [True, True, False, False, True, True]\n",
    "labels = ['batch', 'batch'] # input and output labels\n",
    "#@tf.function(...)\n",
    "\n",
    "#def ...\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('epoch = {}'.format(i))\n",
    "    for j, name in enumerate(trainfiles):\n",
    "        # import batch data\n",
    "        temp_batch = np.load(name, allow_pickle=True)[()]\n",
    "        X = temp_batch['batch']\n",
    "        \n",
    "        # D training\n",
    "        D.trainable = True\n",
    "        g_in = X[..., input_flag]\n",
    "        g_out = G.predict([g_in]) # <-- np.array\n",
    "\n",
    "        d_in_fake = np.concatenate((g_out, g_in), axis=-1) # channel last\n",
    "        d_in_true = X[..., inout_flag]\n",
    "        d_in = np.concatenate((d_in_fake, d_in_true), axis=0) # batch size doubled\n",
    "        d_target = dummy_mix\n",
    "        d_shuffle_ind = du.shuffle_ind(2*batch_size)\n",
    "        \n",
    "        d_loss = D.train_on_batch(d_in[d_shuffle_ind, ...], d_target[d_shuffle_ind, ...])\n",
    "        loss_hist[epochs*i+]\n",
    "#         # G training / transferring\n",
    "#         D.trainable = False\n",
    "#         gan_in = X[..., input_flag]\n",
    "#         gan_target = [X[..., output_flag], dummy_good]\n",
    "        \n",
    "#         gan_loss = GAN.train_on_batch(gan_in, gan_target)\n",
    "        if j%100 == 0:\n",
    "            print('\\t{} step loss = {}'.format(j, d_loss))\n",
    "            \n",
    "D.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99752402e-01, 5.96046448e-08],\n",
       "       [9.98400152e-01, 3.57627869e-07],\n",
       "       [9.99999940e-01, 0.00000000e+00],\n",
       "       [9.99885857e-01, 8.94069672e-08],\n",
       "       [9.99999642e-01, 0.00000000e+00],\n",
       "       [9.99994278e-01, 0.00000000e+00],\n",
       "       [9.99684751e-01, 1.78813934e-07],\n",
       "       [9.99998450e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999285e-01, 0.00000000e+00],\n",
       "       [9.99972045e-01, 0.00000000e+00],\n",
       "       [9.98746514e-01, 3.57627869e-07],\n",
       "       [9.99801993e-01, 2.05636024e-06],\n",
       "       [9.99818087e-01, 1.19209290e-07],\n",
       "       [9.99967217e-01, 0.00000000e+00],\n",
       "       [9.99992788e-01, 2.98023224e-08],\n",
       "       [9.99933004e-01, 0.00000000e+00],\n",
       "       [9.99992847e-01, 0.00000000e+00],\n",
       "       [9.99999404e-01, 0.00000000e+00],\n",
       "       [9.99999702e-01, 0.00000000e+00],\n",
       "       [9.99998093e-01, 1.78813934e-07],\n",
       "       [9.99996305e-01, 8.94069672e-08],\n",
       "       [9.99991298e-01, 0.00000000e+00],\n",
       "       [9.99998212e-01, 0.00000000e+00],\n",
       "       [9.99952793e-01, 0.00000000e+00],\n",
       "       [9.99947011e-01, 0.00000000e+00],\n",
       "       [9.99951303e-01, 2.98023224e-08],\n",
       "       [9.99995291e-01, 5.96046448e-08],\n",
       "       [9.99994576e-01, 2.98023224e-08],\n",
       "       [9.41190362e-01, 3.17674875e-03],\n",
       "       [9.96191502e-01, 1.10298395e-04],\n",
       "       [9.99999523e-01, 0.00000000e+00],\n",
       "       [9.99894738e-01, 8.94069672e-08],\n",
       "       [9.99368191e-01, 4.76837158e-07],\n",
       "       [9.99998271e-01, 2.98023224e-08],\n",
       "       [9.99917388e-01, 2.98023224e-08],\n",
       "       [9.99999762e-01, 0.00000000e+00],\n",
       "       [9.96935248e-01, 2.41100788e-05],\n",
       "       [9.99999404e-01, 2.98023224e-08],\n",
       "       [7.11012244e-01, 4.43875790e-03],\n",
       "       [9.86111403e-01, 1.79111958e-05],\n",
       "       [9.99979615e-01, 0.00000000e+00],\n",
       "       [9.99959469e-01, 0.00000000e+00],\n",
       "       [9.99998987e-01, 2.98023224e-08],\n",
       "       [9.99973595e-01, 0.00000000e+00],\n",
       "       [9.99646187e-01, 1.19209290e-07],\n",
       "       [9.99999523e-01, 0.00000000e+00],\n",
       "       [9.99970913e-01, 2.98023224e-08],\n",
       "       [9.99996901e-01, 8.94069672e-08],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99975443e-01, 1.19209290e-07],\n",
       "       [9.99997675e-01, 0.00000000e+00],\n",
       "       [9.99020934e-01, 1.01029873e-05],\n",
       "       [9.99873877e-01, 8.94069672e-08],\n",
       "       [9.99939799e-01, 5.96046448e-08],\n",
       "       [9.99999464e-01, 0.00000000e+00],\n",
       "       [9.99999464e-01, 5.96046448e-08],\n",
       "       [9.99959707e-01, 0.00000000e+00],\n",
       "       [9.99988317e-01, 0.00000000e+00],\n",
       "       [9.99449909e-01, 3.87430191e-07],\n",
       "       [9.99984086e-01, 0.00000000e+00],\n",
       "       [9.99684989e-01, 1.19209290e-07],\n",
       "       [9.99999285e-01, 2.98023224e-08],\n",
       "       [9.91881132e-01, 5.11705875e-05],\n",
       "       [9.99999225e-01, 1.19209290e-07],\n",
       "       [9.99780655e-01, 1.63912773e-06],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99956191e-01, 0.00000000e+00],\n",
       "       [9.99999046e-01, 5.96046448e-08],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99967992e-01, 8.94069672e-08],\n",
       "       [9.98098135e-01, 1.04308128e-06],\n",
       "       [9.99766290e-01, 1.19209290e-07],\n",
       "       [9.99997914e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 5.96046448e-08],\n",
       "       [9.99993086e-01, 0.00000000e+00],\n",
       "       [9.99995470e-01, 2.98023224e-08],\n",
       "       [9.99276280e-01, 1.37388706e-05],\n",
       "       [9.99960065e-01, 8.94069672e-08],\n",
       "       [9.99777496e-01, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [9.95792568e-01, 8.04662704e-07],\n",
       "       [9.99993086e-01, 6.85453415e-07],\n",
       "       [9.99997258e-01, 5.96046448e-08],\n",
       "       [9.99930501e-01, 1.49011612e-07],\n",
       "       [9.99999523e-01, 0.00000000e+00],\n",
       "       [9.99816597e-01, 0.00000000e+00],\n",
       "       [9.99642849e-01, 1.19209290e-07],\n",
       "       [9.98430610e-01, 6.25848770e-07],\n",
       "       [9.97749448e-01, 0.00000000e+00],\n",
       "       [9.98165488e-01, 1.19209290e-07],\n",
       "       [9.99132872e-01, 1.63912773e-05],\n",
       "       [9.99980986e-01, 0.00000000e+00],\n",
       "       [9.99799252e-01, 1.55866146e-05],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999642e-01, 0.00000000e+00],\n",
       "       [9.92973328e-01, 1.71661377e-05],\n",
       "       [9.99996066e-01, 5.96046448e-08],\n",
       "       [9.57808733e-01, 3.51816416e-04],\n",
       "       [9.98793364e-01, 2.98023224e-08],\n",
       "       [9.99819100e-01, 8.94069672e-08],\n",
       "       [9.99887466e-01, 0.00000000e+00],\n",
       "       [9.99923825e-01, 8.34465027e-07],\n",
       "       [9.94106114e-01, 2.27689743e-05],\n",
       "       [9.82647181e-01, 4.87864017e-05],\n",
       "       [9.99999285e-01, 0.00000000e+00],\n",
       "       [9.99978065e-01, 0.00000000e+00],\n",
       "       [9.97481883e-01, 8.94069672e-08],\n",
       "       [9.99971151e-01, 1.49011612e-07],\n",
       "       [9.99995708e-01, 0.00000000e+00],\n",
       "       [9.99991894e-01, 1.46031380e-06],\n",
       "       [9.99999642e-01, 0.00000000e+00],\n",
       "       [9.99866903e-01, 8.94069672e-08],\n",
       "       [9.99999285e-01, 0.00000000e+00],\n",
       "       [9.99983311e-01, 1.49011612e-07],\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [9.99999166e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999642e-01, 0.00000000e+00],\n",
       "       [9.98972356e-01, 8.94069672e-08],\n",
       "       [9.99682546e-01, 2.98023224e-08],\n",
       "       [9.99993324e-01, 8.94069672e-08],\n",
       "       [9.99941051e-01, 2.98023224e-08],\n",
       "       [9.99999285e-01, 8.94069672e-08],\n",
       "       [9.98821974e-01, 8.94069672e-08],\n",
       "       [9.99971151e-01, 0.00000000e+00],\n",
       "       [9.96867895e-01, 1.28149986e-05],\n",
       "       [9.99980569e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [8.11651766e-01, 2.32521594e-02],\n",
       "       [9.99725044e-01, 1.22189522e-06],\n",
       "       [9.99941111e-01, 5.96046448e-08],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999285e-01, 0.00000000e+00],\n",
       "       [9.99562681e-01, 1.19209290e-07],\n",
       "       [9.99774516e-01, 2.98023224e-08],\n",
       "       [9.98535812e-01, 5.36441803e-07],\n",
       "       [9.99967337e-01, 0.00000000e+00],\n",
       "       [9.99986529e-01, 0.00000000e+00],\n",
       "       [9.99999762e-01, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [9.99998927e-01, 8.94069672e-08],\n",
       "       [9.99993682e-01, 0.00000000e+00],\n",
       "       [9.99902010e-01, 2.98023224e-08],\n",
       "       [9.99902964e-01, 2.98023224e-08],\n",
       "       [9.99992788e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999285e-01, 0.00000000e+00],\n",
       "       [9.99956250e-01, 5.96046448e-08],\n",
       "       [9.99999106e-01, 1.19209290e-07],\n",
       "       [9.99991417e-01, 0.00000000e+00],\n",
       "       [9.85911846e-01, 8.07642937e-05],\n",
       "       [9.99998093e-01, 3.27825546e-07],\n",
       "       [9.99901950e-01, 5.96046448e-08],\n",
       "       [9.99873221e-01, 1.19209290e-07],\n",
       "       [9.99835849e-01, 1.19209290e-06],\n",
       "       [9.99995112e-01, 0.00000000e+00],\n",
       "       [9.99980211e-01, 0.00000000e+00],\n",
       "       [9.99941349e-01, 8.64267349e-07],\n",
       "       [9.99999046e-01, 0.00000000e+00],\n",
       "       [9.67733860e-01, 1.66803598e-04],\n",
       "       [9.99999881e-01, 2.98023224e-08],\n",
       "       [9.99998212e-01, 0.00000000e+00],\n",
       "       [8.06286693e-01, 5.68515062e-03],\n",
       "       [9.99969423e-01, 8.94069672e-08],\n",
       "       [9.99999523e-01, 0.00000000e+00],\n",
       "       [9.99999881e-01, 0.00000000e+00],\n",
       "       [9.99999464e-01, 0.00000000e+00],\n",
       "       [9.99236166e-01, 1.07288361e-06],\n",
       "       [9.99997258e-01, 0.00000000e+00],\n",
       "       [9.99997258e-01, 1.78813934e-07],\n",
       "       [9.99972701e-01, 2.98023224e-08],\n",
       "       [9.99998808e-01, 5.96046448e-07],\n",
       "       [8.93771768e-01, 1.61501765e-03],\n",
       "       [9.99993861e-01, 5.96046448e-08],\n",
       "       [8.85503471e-01, 5.34713268e-04],\n",
       "       [9.99889612e-01, 8.94069672e-08],\n",
       "       [9.97956157e-01, 5.45382500e-06],\n",
       "       [9.99998331e-01, 0.00000000e+00],\n",
       "       [9.99946833e-01, 2.98023224e-08],\n",
       "       [9.99975204e-01, 4.76837158e-07],\n",
       "       [9.99324977e-01, 3.87430191e-07],\n",
       "       [9.99771953e-01, 2.98023224e-08],\n",
       "       [9.99982595e-01, 0.00000000e+00],\n",
       "       [1.00000000e+00, 0.00000000e+00],\n",
       "       [9.99972761e-01, 2.98023224e-08],\n",
       "       [9.99996781e-01, 5.96046448e-08],\n",
       "       [9.99995351e-01, 0.00000000e+00],\n",
       "       [9.99999583e-01, 3.57627869e-07],\n",
       "       [9.99997973e-01, 2.98023224e-08],\n",
       "       [9.99454916e-01, 3.57627869e-07],\n",
       "       [9.99981880e-01, 2.98023224e-08],\n",
       "       [9.99961615e-01, 2.98023224e-08],\n",
       "       [9.99997377e-01, 0.00000000e+00],\n",
       "       [9.92916226e-01, 1.19209290e-07],\n",
       "       [9.99952316e-01, 1.49011612e-07],\n",
       "       [9.99998033e-01, 0.00000000e+00],\n",
       "       [9.99532223e-01, 0.00000000e+00],\n",
       "       [9.99958277e-01, 0.00000000e+00],\n",
       "       [9.99998093e-01, 0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.predict(d_in_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
